{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "# Define the base model\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout layer for regularization\n",
    "predictions = Dense(5, activation='softmax')(x)  # Update the number of units to 3\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Preprocess videos and generate training data\n",
    "video_dir = 'Datasett/Video'\n",
    "categories = ['Programming', 'Sejarah', 'Matematika', 'Seni', 'Sains']\n",
    "frames_per_video = 10  # Number of frames to extract per video\n",
    "\n",
    "def video_generator(video_dir, categories, frames_per_video, batch_size=45):\n",
    "    while True:\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for category_idx, category in enumerate(categories):\n",
    "            category_path = os.path.join(video_dir, category)\n",
    "            video_files = os.listdir(category_path)\n",
    "\n",
    "            for video_file in video_files:\n",
    "                video_path = os.path.join(category_path, video_file)\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "                # Extract frames from video\n",
    "                frame_count = 0\n",
    "                frames = []\n",
    "                while cap.isOpened() and frame_count < frames_per_video:\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        frame = cv2.resize(frame, (128, 128))\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        frames.append(frame)\n",
    "                        frame_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                cap.release()\n",
    "\n",
    "                if len(frames) == frames_per_video:\n",
    "                    X_train.extend(frames)\n",
    "                    y_train.extend([category_idx] * frames_per_video)\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "\n",
    "        # Normalize input data\n",
    "        X_train = X_train / 255.0\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            yield X_train[batch_indices], y_train[batch_indices]\n",
    "\n",
    "# Preprocess validation data\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "for category_idx, category in enumerate(categories):\n",
    "    category_path = os.path.join(video_dir, category)\n",
    "    video_files = os.listdir(category_path)\n",
    "\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(category_path, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_count = 0\n",
    "        frames = []\n",
    "        while cap.isOpened() and frame_count < frames_per_video:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, (128, 128))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                frame_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == frames_per_video:\n",
    "            valid_data.extend(frames)\n",
    "            valid_labels.extend([category_idx] * frames_per_video)\n",
    "\n",
    "valid_data = np.array(valid_data)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "valid_labels = tf.keras.utils.to_categorical(valid_labels, num_classes=len(categories))\n",
    "\n",
    "# Normalize validation data\n",
    "valid_data = valid_data / 255.0\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "train_generator = video_generator(video_dir, categories, frames_per_video, batch_size=16)\n",
    "num_train_samples = sum(len(files) for _, _, files in os.walk(video_dir))\n",
    "steps_per_epoch = num_train_samples // 16\n",
    "validation_data = (valid_data, valid_labels)\n",
    "\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, validation_data=validation_data, epochs=10, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('video_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('video_classification_model.h5')\n",
    "\n",
    "# Define the function to preprocess a single frame\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, (128, 128))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "# Define the function to preprocess a video for testing\n",
    "def preprocess_video(video_path, frames_per_video):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < frames_per_video:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            preprocessed_frame = preprocess_frame(frame)\n",
    "            frames.append(preprocessed_frame)\n",
    "            frame_count += 1\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) == frames_per_video:\n",
    "        return np.array(frames)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define the video path and number of frames per video\n",
    "video_path = 'Download.mp4'\n",
    "frames_per_video = 10\n",
    "\n",
    "# Preprocess the video frames\n",
    "preprocessed_video = preprocess_video(video_path, frames_per_video)\n",
    "\n",
    "if preprocessed_video is not None:\n",
    "    # Make predictions on each frame individually\n",
    "    predictions = []\n",
    "    for frame in preprocessed_video:\n",
    "        frame = np.reshape(frame, (1, 128, 128, 3))\n",
    "        frame_prediction = model.predict(frame)\n",
    "        predictions.append(frame_prediction)\n",
    "        \n",
    "\n",
    "    # Average the predictions for all frames\n",
    "    predictions = np.array(predictions)\n",
    "    average_prediction = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(average_prediction)\n",
    "\n",
    "    # Create a mapping of class indices to labels\n",
    "    class_labels = ['Python', 'Sejarah', 'Matematika']\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    print(\"Predicted Class:\", predicted_class_label)\n",
    "else:\n",
    "    print(\"Unable to preprocess the video frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62ca33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\patri\\AppData\\Local\\Temp\\tmp6x1_sl4t\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\patri\\AppData\\Local\\Temp\\tmp6x1_sl4t\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Path to your Keras model file (.h5)\n",
    "model_path = 'DatasetYoutubes/speech_recognition.h5'\n",
    "\n",
    "# Output path for the TFLite model\n",
    "tflite_model_path = 'DatasetYoutubes/speech_recognition.tflite'\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Include default supported ops\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Include TensorFlow Select ops\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to disk\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print('TFLite model conversion complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1089826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
